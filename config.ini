[DEFAULT]

job_dir = ./jobs
output_dir = ./jobs/out
	   
batch_size = 64
epochs = 200
test_sample_size = 1024
validation = 8192
test_batch_size = 128

weight_decay = 3e-5
grad_clipping = 100

type = cvae

dataset = mnist
transformer = simple

optimizer = adam
batch_norm = both

sigma = 0.1

test_latent_sampling = 128
latent_sampling = 1

latent_dim = 256
prior_variance = scalar
prior_means = 0
learned_prior_means = True

features = none
encoder = 512 256
decoder = 256 512
upsampler =

activation = relu
output_activation = sigmoid

classifier = 

gamma = 1000

[mnist-dense]

sigma = learned

encoder = 1024 512 512
decoder = 512 512 1024


[mnist-vgg]

transformer = pad

sigma = learned

features = vgg11
encoder = 512 256
decoder = 256 512
upsampler =  512 256 128 64 1

test_batch_size = 128

[tilted]

batch_size = 64
epochs = 250

lr = 1e-4
weight_decay = 3e-5
grad_clipping = 100

test_sample_size = 1024
validation = 0

type = vae

dataset = cifar10
transformer = simple

optimizer = adam
batch_norm = none

sigma = 0.7071

test_latent_sampling = 1
latent_sampling = 1

latent_dim = 100
prior = tilted
prior_variance = scalar
prior_means = 25
tilted_tau = 25
learned_prior_means = False

encoder_forced_variance = 1.

features = conv32
encoder = 
decoder = 
upsampler = deconv32

activation = leaky
output_activation = linear

gamma = 0


[imagenet]

dataset = imagenet21k
transformer = crop
data_augmentation = flip crop

epochs = 600

sigma = learned

features = vgg19
encoder = 4096 4096
decoder = 4096 4096
upsampler = 512 256 128 64 3

test_batch_size = 32


[cifar10]

dataset = cifar10
transformer = simple

optimizer = adam
lr = 1e-4
weight_decay = 3e-5
grad_clipping = 100

test_sample_size = 1024
validation = 1024

batch_norm = none

epochs = 600
latent_dim = 256

prior_variance = scalar
prior_means = 0

learned_prior_means = True

features = conv32
encoder = 
decoder = 
upsampler = deconv32

sigma = learned

activation = leaky
output_activation = linear

gamma = 0


[cifar10-vgg]

dataset = cifar10
# data_augmentation = flip crop

epochs = 600

sigma = learned

features = vgg19
encoder = 
decoder = 
upsampler = 512 256 128 64 3

test_batch_size = 128


[svhn]

data_augmentation = crop

epochs = 500
dataset = svhn

sigma = learned

features = vgg16
encoder = 
decoder = 
upsampler =  512 256 128 64 3

test_batch_size = 128


[fashion]

dataset = fashion
transformer = pad

test_batch_size = 128

epochs = 250
validation = 2048

sigma = learned

features = vgg11
encoder = 512 256

latent_dim = 512
latent_sampling = 1

decoder = 256 512 
# upsampler =  512 256 128 64 1
upsampler = [x4:2+1]512-256-128-64-1

[cifar-ola]
# from olaralex

sigma = 0.05
latent_dim = 1024

features = conv
features_channels = 64 128 512
encoder = 

decoder = 1024

upsampler = 256 64 3


[dai-iclr20]

latent_dim = 512

features = conv
conv_padding = 1
features_channels = 64 128 256

encoder = 

decoder =
# 4096

upsampler = 128 64 3

[dai-iclr20x8]

latent_dim = 512

features = conv
conv_padding = 1
features_channels = 512 1024 256

encoder = 

decoder =
# 4096

upsampler = 1024 512 3


[fashion-vgg16]

dataset = fashion
transformer = pad

features = vgg16

encoder = 512 256

latent_dim = 1024
latent_sampling = 1

decoder = 256 512 
upsampler =  512 256 128 64 1

classifier = 20 10


[mnist-5A]

dataset = fashion
transformer = pad

features = vgg
features_channels = 64 A 128 A 256 256 A 512 512 A 512 512 A

encoder = 512 256

latent_dim = 1024
latent_sampling = 1

decoder = 256 512 
upsampler =  512 256 128 64 1

classifier = 20 10


[autoencoder]

transformer = pad

latent_sampling = 1
latent_dim = 256
sigma = 0
encoder =  512 512
decoder = 512 512
features = conv
conv_padding = 1
features_channels = 16 64
upsampler = 16 3


[vgg-baseline]

type = vib
latent_sampling = 1
latent_dim = 512
sigma = 0
encoder = 
decoder =
classifier = 
features = vgg11
batch_norm = True


[bogus]

coder_means = onehot
latent_sampling = 3
test_batch_size = 1

[wim-default]
output_dir = ./jobs/out
source_job_dir = ./jobs
target_job_dir = ./wim-jobs
prior_means = 0.
tau = 25

alpha = 0.1
wim_epochs = 5
test_batch_size = 32
